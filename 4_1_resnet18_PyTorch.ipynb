{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F21E3CbhJ8N"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiTLir6FhJ8S"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbbp388ehJ8T"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMFCOrRehJ8T"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions:\n",
        "\n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7hjp0CqhJ8U"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGkXdPophJ8V"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"https://#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"https://#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"https://#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"https://#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"https://#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"https://#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypNnVoeChJ8W"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkhre_UphJ8W"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oU3eC-hhJ8X",
        "outputId": "d0637213-d4d3-4087-e76b-3f7ed56d850d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-22 23:24:57--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  37.6MB/s    in 72s     \n",
            "\n",
            "2022-09-22 23:26:09 (34.4 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J15WX3gBhJ8Y"
      },
      "outputs": [],
      "source": [
        "!unzip -q Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ci8OueZohJ8Y",
        "outputId": "edeacffd-992f-4fb5-eedf-cf89959119b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-22 23:28:07--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  31.4MB/s    in 58s     \n",
            "\n",
            "2022-09-22 23:29:06 (34.5 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PExBLalhJ8Y"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OsLf4cRhJ8Z",
        "outputId": "eb29970d-7761-4998-bea2-1a4c60083e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxdDyvbmhJ8Z"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbN1G9dLhJ8Z"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W740ANPyhJ8Z",
        "outputId": "cfd6e18a-71d6-4471-f892-a99d0059d868"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa4376b6830>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avsE7pKChJ8a"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDIVLwwChJ8a"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJNQg1hjhJ8a"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIkjkIeRhJ8a"
      },
      "source": [
        "This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEckc3ZkhJ8a",
        "outputId": "56e55946-2642-4389-fad3-a04e76bba13f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/content\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyB5KxTphJ8b"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs-7GAFshJ8b",
        "outputId": "0efe9b82-44a3-452b-967f-8e8262f45518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeW28_iAhJ8b"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih5Wj_GohJ8b"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPz9qbfnhJ8b"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A7TJ9R7_hJ8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "1a3fdf63453046d6906d155fef600343",
            "46e7635a773446819dd65de3b39ae76f",
            "04eed0422c88480da282cb36208d9730",
            "4dec4be0d37443088faedffc28ccb9bf",
            "1e7a332d915a4250859a2543dfe9f786",
            "fe5db627e00d4944a86703a257744804",
            "86a3241fb00e4ef88336349a6d1f1b21",
            "9a122b0fe54a4e759bb76ee5947ed026",
            "627ec2d4b1864a0ab4d985b96973723b",
            "ac8ded8b1e6e48dfb2b61777543f837a",
            "526012410c4940cd97f772c572400d8e"
          ]
        },
        "outputId": "3d87df74-916a-4258-9462-e0d46285c7e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a3fdf63453046d6906d155fef600343"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "model= models.resnet18(pretrained=True)\n",
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOJuOs2ShJ8b"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cmW9naORhJ8c"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "for param in model.parameters():\n",
        "  param.requires_grad=False\n",
        "  model.fc=nn.Linear(512,7)\n",
        "\n",
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCGdrPTNhJ8c"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTzHEoj7hJ8c"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G0I_QrqYhJ8c"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512,7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDh7YJr6hJ8c"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OwjFlfq5hJ8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38e7992-355e-422f-c475-c6b58afbdff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN0KWIzmhJ8c"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuCuXRE6hJ8c"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjHQb0cghJ8c"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NSEshMWAhJ8c"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Type your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72s6XdPhhJ8d"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "938kuLUbhJ8d"
      },
      "outputs": [],
      "source": [
        "train_loader=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
        "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPNtBkFIhJ8d"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nmNsIZZRhJ8d"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N7cR3V5hJ8d"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgzIGOg8hJ8d"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tGwxRaUWhJ8d"
      },
      "outputs": [],
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        model.train() \n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()\n",
        "        #make a prediction \n",
        "        z = model(x)\n",
        "        # calculate loss \n",
        "        loss = criterion(z, y)\n",
        "        # calculate gradients of parameters \n",
        "        loss.backward()\n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.data)\n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        model.eval()\n",
        "        #make a prediction \n",
        "        z = model(x_test)\n",
        "        #find max \n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "       \n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        correct += (yhat == y_test).sum().item()\n",
        "   \n",
        "    accuracy=correct/N_test\n",
        "    accuracy_list.append(accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N96CaWeDhJ8d"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1KxIPK2OhJ8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd1e0f9-b40d-41b3-e8e4-c4cd4e783895"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.994"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ussX6m17hJ8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "8faea219-2b00-42e5-a3a3-ab24a51fcc80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhiZwjwahJ8e"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2HZipgxhJ8e"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "TGs-5cN-hJ8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a1b3a15-f06c-4693-c98b-7026e7afc5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample : 16; Expected Label: tensor([1]); Obtained Label: tensor([0])\n",
            "Sample : 185; Expected Label: tensor([0]); Obtained Label: tensor([1])\n",
            "Sample : 788; Expected Label: tensor([1]); Obtained Label: tensor([0])\n",
            "Sample : 928; Expected Label: tensor([1]); Obtained Label: tensor([0])\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "max_num_of_items = 4  # first four mis-classified samples\n",
        "validation_loader_batch_one = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1)\n",
        "\n",
        "for i, (x_test, y_test) in enumerate(validation_loader_batch_one):\n",
        "    # set model to eval\n",
        "    model.eval()\n",
        "    \n",
        "    # make a prediction\n",
        "    z = model(x_test)\n",
        "    \n",
        "    # find max\n",
        "    _, yhat = torch.max(z.data, 1)\n",
        "    \n",
        "    # print mis-classified samples\n",
        "    if yhat != y_test:\n",
        "        print(\"Sample : {}; Expected Label: {}; Obtained Label: {}\".format(str(i), str(y_test), str(yhat)))\n",
        "        count += 1\n",
        "        if count >= max_num_of_items:\n",
        "            break\n",
        "    # end if\n",
        "# end for   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgGhgJzchJ8e"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj8XpsPFhJ8e"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cyGj5jfhJ8e"
      },
      "source": [
        "## Change Log\n",
        "\n",
        "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
        "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
        "| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieHhJNvrhJ8e"
      },
      "source": [
        "Copyright © 2018 <a href=\"https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a3fdf63453046d6906d155fef600343": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46e7635a773446819dd65de3b39ae76f",
              "IPY_MODEL_04eed0422c88480da282cb36208d9730",
              "IPY_MODEL_4dec4be0d37443088faedffc28ccb9bf"
            ],
            "layout": "IPY_MODEL_1e7a332d915a4250859a2543dfe9f786"
          }
        },
        "46e7635a773446819dd65de3b39ae76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe5db627e00d4944a86703a257744804",
            "placeholder": "​",
            "style": "IPY_MODEL_86a3241fb00e4ef88336349a6d1f1b21",
            "value": "100%"
          }
        },
        "04eed0422c88480da282cb36208d9730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a122b0fe54a4e759bb76ee5947ed026",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_627ec2d4b1864a0ab4d985b96973723b",
            "value": 46830571
          }
        },
        "4dec4be0d37443088faedffc28ccb9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac8ded8b1e6e48dfb2b61777543f837a",
            "placeholder": "​",
            "style": "IPY_MODEL_526012410c4940cd97f772c572400d8e",
            "value": " 44.7M/44.7M [00:02&lt;00:00, 17.4MB/s]"
          }
        },
        "1e7a332d915a4250859a2543dfe9f786": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe5db627e00d4944a86703a257744804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86a3241fb00e4ef88336349a6d1f1b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a122b0fe54a4e759bb76ee5947ed026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "627ec2d4b1864a0ab4d985b96973723b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac8ded8b1e6e48dfb2b61777543f837a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "526012410c4940cd97f772c572400d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}